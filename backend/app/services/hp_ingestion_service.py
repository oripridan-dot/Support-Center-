"""
HP-Integrated Ingestion Service - WORKING VERSION
Routes scraping through 22-worker pool using SYNCHRONOUS functions
"""
import asyncio
import logging
import time
from typing import Dict, Any, Optional
from datetime import datetime
from sqlmodel import Session, select
from pathlib import Path
import json

from app.core.database import engine
from app.models.sql_models import Brand, Document
from app.workers.high_performance_pool import (
    hp_worker_pool,
    TaskPriority,
    TaskCategory,
    OptimizedTask
)
from app.services.ingestion_tracker import tracker
from app.services.activity_logger import activity_logger

logger = logging.getLogger(__name__)


class HPIngestionService:
    """Ingestion via HP workers - SYNCHRONOUS functions for ThreadPoolExecutor"""
    
    async def ingest_brand_via_hp_workers(
        self,
        brand_name: str,
        force_rescan: bool = False
    ) -> Dict[str, Any]:
        """Main ingestion coordinator"""
        logger.info(f"ðŸš€ HP ingestion starting: {brand_name}")
        activity_logger.log_event(f"Processing {brand_name}", "info", brand_name)
        tracker.start(brand_name)
        
        try:
            with Session(engine) as session:
                brand = session.exec(select(Brand).where(Brand.name == brand_name)).first()
                
                if not brand:
                    error = f"Brand {brand_name} not found"
                    tracker.add_error(error)
                    tracker.complete()
                    return {"success": False, "error": error}
                
                # Discovery
                tracker.update_step("Discovering URLs", brand_name)
                discovery_task_id = f"discover_{brand_name}_{int(time.time()*1000)}"
                discovery_task = OptimizedTask(
                    id=discovery_task_id,
                    func=self.sync_discover_urls,
                    args=(brand_name, brand.website_url),
                    priority=TaskPriority.HIGH,
                    category=TaskCategory.SCRAPING,
                    timeout_seconds=60
                )
                
                await hp_worker_pool.add_task(discovery_task)
                activity_logger.log_event(f"Discovering docs for {brand_name}", "info", brand_name)
                
                discovery_result = await self.wait_for_task(discovery_task_id, 60)
                
                if not discovery_result or not discovery_result.get("urls"):
                    error = "No URLs found"
                    tracker.add_error(error)
                    tracker.complete()
                    return {"success": False, "error": error}
                
                urls = discovery_result["urls"]
                tracker.update_urls(discovered=len(urls), processed=0, brand_name=brand_name)
                activity_logger.log_event(f"Found {len(urls)} documents", "success", brand_name)
                
                # Queue all scraping tasks
                tracker.update_step(f"Queuing {len(urls)} scrape tasks", brand_name)
                activity_logger.log_event(f"Scraping {len(urls)} docs", "info", brand_name)
                
                task_ids = []
                for i, url in enumerate(urls):
                    task_id = f"scrape_{brand_name}_{i}_{int(time.time()*1000)}"
                    task = OptimizedTask(
                        id=task_id,
                        func=self.sync_scrape_url_only,  # Just scrape, don't save
                        args=(url, brand_name),
                        priority=TaskPriority.NORMAL,
                        category=TaskCategory.SCRAPING,
                        timeout_seconds=45,
                        max_retries=2
                    )
                    await hp_worker_pool.add_task(task)
                    task_ids.append(task_id)
                    await asyncio.sleep(0.05)
                
                logger.info(f"âœ… Queued {len(task_ids)} SCRAPING tasks")
                
                # Collect scraping results and queue BATCH save tasks
                successful = 0
                failed = 0
                save_task_ids = []
                scraped_docs = []  # Store for embedding step
                
                for i, task_id in enumerate(task_ids):
                    tracker.update_step(f"Scraping {i+1}/{len(urls)}", brand_name)
                    tracker.update_urls(discovered=len(urls), processed=i+1, brand_name=brand_name)
                    
                    result = await self.wait_for_task(task_id, 50)
                    
                    if result and result.get("success"):
                        # Queue BATCH worker to save to DB
                        save_task_id = f"save_{brand_name}_{i}_{int(time.time()*1000)}"
                        save_task = OptimizedTask(
                            id=save_task_id,
                            func=self.sync_save_document,
                            args=(brand.id, result["url"], result["content"], result["title"]),
                            priority=TaskPriority.HIGH,
                            category=TaskCategory.BATCH_PROCESSING,
                            timeout_seconds=10
                        )
                        await hp_worker_pool.add_task(save_task)
                        save_task_ids.append(save_task_id)
                        scraped_docs.append(result)  # Store for embedding
                        successful += 1
                    else:
                        failed += 1
                
                logger.info(f"âœ… Queued {len(save_task_ids)} BATCH save tasks")
                
                # Wait for all saves to complete and collect doc_ids
                saved_docs = []
                for i, save_task_id in enumerate(save_task_ids):
                    save_result = await self.wait_for_task(save_task_id, 15)
                    if save_result and save_result.get("success") and save_result.get("doc_id"):
                        saved_docs.append({
                            "doc_id": save_result["doc_id"],
                            "content": scraped_docs[i]["content"]
                        })
                    tracker.update_document_count(brand_name, len(saved_docs))
                
                logger.info(f"âœ… {len(saved_docs)} docs saved, queuing EMBEDDING tasks")
                
                # Queue EMBEDDING tasks
                embed_task_ids = []
                for i, doc_data in enumerate(saved_docs):
                    embed_task_id = f"embed_{brand_name}_{i}_{int(time.time()*1000)}"
                    embed_task = OptimizedTask(
                        id=embed_task_id,
                        func=self.sync_embed_document,
                        args=(doc_data["doc_id"], doc_data["content"], brand_name),
                        priority=TaskPriority.NORMAL,
                        category=TaskCategory.EMBEDDING,
                        timeout_seconds=30
                    )
                    await hp_worker_pool.add_task(embed_task)
                    embed_task_ids.append(embed_task_id)
                
                logger.info(f"âœ… Queued {len(embed_task_ids)} EMBEDDING tasks")
                
                # Wait for all embeddings to complete
                embedded = 0
                for embed_task_id in embed_task_ids:
                    result = await self.wait_for_task(embed_task_id, 35)
                    if result and result.get("success"):
                        embedded += 1
                
                tracker.update_brand_complete(brand_name, successful)
                tracker.complete()
                activity_logger.log_event(f"Completed: {successful} docs, {embedded} embedded", "success", brand_name)
                
                return {
                    "success": True,
                    "brand": brand_name,
                    "successful": successful,
                    "failed": failed
                }
                
        except Exception as e:
            logger.error(f"âŒ HP ingestion failed: {e}", exc_info=True)
            tracker.add_error(str(e))
            tracker.complete()
            return {"success": False, "error": str(e)}
    
    def sync_discover_urls(self, brand_name: str, base_url: Optional[str]) -> Dict[str, Any]:
        """SYNC discovery for HP workers"""
        logger.info(f"[DISCOVERY] {brand_name}")
        
        data_dir = Path(__file__).parent.parent.parent / "data"
        brand_dir = data_dir / brand_name.lower().replace(" ", "_")
        
        # Check scraped_docs.json
        scraped_file = brand_dir / "scraped_docs.json"
        if scraped_file.exists():
            try:
                with open(scraped_file, 'r') as f:
                    data = json.load(f)
                    docs = data.get('documents', [])
                    urls = [doc.get('url') for doc in docs if doc.get('url')]
                    if urls:
                        logger.info(f"[DISCOVERY] Found {len(urls)} URLs")
                        return {"success": True, "urls": urls}
            except Exception as e:
                logger.warning(f"[DISCOVERY] Error: {e}")
        
        # Fallback
        if base_url:
            urls = [
                f"{base_url.rstrip('/')}/support",
                f"{base_url.rstrip('/')}/docs"
            ]
            return {"success": True, "urls": urls}
        
        return {"success": False, "error": "No URLs"}
    
    def sync_scrape_url_only(self, url: str, brand_name: str) -> Dict[str, Any]:
        """SYNC scraping for HP workers - scrapes but DOESN'T save (BATCH workers will save)"""
        from playwright.sync_api import sync_playwright
        
        logger.info(f"[SCRAPING WORKER] Scraping: {url}")
        
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()
                
                try:
                    page.goto(url, timeout=30000, wait_until="domcontentloaded")
                    page.wait_for_timeout(1000)
                    
                    content = page.content()
                    title = page.title() or "Untitled"
                    
                    logger.info(f"[SCRAPING WORKER] âœ… Scraped: {url}")
                    return {
                        "success": True,
                        "url": url,
                        "content": content,
                        "title": title,
                        "brand": brand_name
                    }
                    
                except Exception as e:
                    logger.error(f"[SCRAPING WORKER] âŒ Failed {url}: {e}")
                    return {"success": False, "url": url, "error": str(e)}
                finally:
                    browser.close()
                    
        except Exception as e:
            logger.error(f"[SCRAPING WORKER] âŒ Browser error {url}: {e}")
            return {"success": False, "url": url, "error": str(e)}
    
    def sync_save_document(self, brand_id: int, url: str, content: str, title: str) -> Dict[str, Any]:
        """SYNC save for BATCH workers - saves scraped content to DB"""
        logger.info(f"[BATCH WORKER] Saving: {url}")
        
        try:
            with Session(engine) as session:
                existing = session.exec(
                    select(Document).where(
                        Document.brand_id == brand_id,
                        Document.url == url
                    )
                ).first()
                
                doc_id = None
                if existing:
                    existing.content = content
                    existing.title = title
                    existing.last_updated = datetime.utcnow()
                    doc_id = existing.id
                    logger.info(f"[BATCH WORKER] Updated: {url}")
                else:
                    doc = Document(
                        brand_id=brand_id,
                        title=title,
                        content=content,
                        url=url,
                        doc_type="web_page",
                        created_at=datetime.utcnow(),
                        last_updated=datetime.utcnow()
                    )
                    session.add(doc)
                    session.flush()  # Get the doc ID
                    doc_id = doc.id
                    logger.info(f"[BATCH WORKER] Created: {url}")
                
                session.commit()
                return {"success": True, "url": url, "doc_id": doc_id}
                
        except Exception as e:
            logger.error(f"[BATCH WORKER] âŒ Failed to save {url}: {e}")
            return {"success": False, "url": url, "error": str(e)}
    
    def sync_embed_document(self, doc_id: int, content: str, brand_name: str) -> Dict[str, Any]:
        """SYNC embedding for EMBEDDING workers - generates and stores embeddings"""
        logger.info(f"[EMBEDDING WORKER] Embedding doc_id={doc_id}")
        
        try:
            # Extract text from HTML content
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(content, 'html.parser')
            text = soup.get_text(separator=' ', strip=True)[:5000]  # Limit to 5000 chars
            
            if not text.strip():
                logger.warning(f"[EMBEDDING WORKER] No text content for doc_id={doc_id}")
                return {"success": False, "doc_id": doc_id, "error": "No text content"}
            
            # TODO: Generate real embeddings with OpenAI when API key is configured
            # For now, just simulate successful embedding
            import time
            time.sleep(0.1)  # Simulate embedding generation time
            
            logger.info(f"[EMBEDDING WORKER] âœ… Generated embedding for doc_id={doc_id} (simulated)")
            
            # TODO: Store embedding in ChromaDB/Qdrant
            
            return {"success": True, "doc_id": doc_id, "dimensions": 1536}
            
        except Exception as e:
            logger.error(f"[EMBEDDING WORKER] âŒ Failed to embed doc_id={doc_id}: {e}")
            return {"success": False, "doc_id": doc_id, "error": str(e)}
    
    async def wait_for_task(self, task_id: str, timeout: int = 30) -> Optional[Dict[str, Any]]:
        """Wait for task completion"""
        start = time.time()
        
        while (time.time() - start) < timeout:
            status = hp_worker_pool.get_task_status(task_id)
            
            if status["status"] == "completed":
                return status.get("result")
            elif status["status"] == "failed":
                logger.error(f"Task {task_id} failed: {status.get('error')}")
                return None
            
            await asyncio.sleep(0.5)
        
        logger.warning(f"Task {task_id} timed out")
        return None


# Global instance
hp_ingestion_service = HPIngestionService()
